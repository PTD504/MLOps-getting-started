import pandas as pd
import numpy as np
import optuna  # Th√™m Optuna cho tuning hyperparameter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
import mlflow
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os

def objective(trial, X_train, y_train, X_val, y_val, classifier_name):
    """H√†m m·ª•c ti√™u cho Optuna hyperparameter tuning cho m·ªôt classifier c·ªë ƒë·ªãnh"""
    # Kh·ªüi t·∫°o hyperparameters chung
    max_features = trial.suggest_int("max_features", 5000, 20000)
    ngram_range = trial.suggest_categorical("ngram_range", [(1, 1), (1, 2), (1, 3)])
    
    # TF-IDF
    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)
    
    # Classifier v·ªõi hyperparameter ri√™ng t∆∞∆°ng ·ª©ng
    if classifier_name == "logistic_regression":
        C = trial.suggest_float("C", 0.1, 10.0, log=True)
        classifier = LogisticRegression(C=C, max_iter=1000, random_state=42)
    elif classifier_name == "svm":
        C = trial.suggest_float("C", 0.1, 10.0, log=True)
        classifier = LinearSVC(C=C, random_state=42, max_iter=1000)
    elif classifier_name == "naive_bayes":
        alpha = trial.suggest_float("alpha", 0.1, 10.0, log=True)
        classifier = MultinomialNB(alpha=alpha)
    else:
        raise ValueError(f"Classifier {classifier_name} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
    
    # T·∫°o pipeline
    pipeline = Pipeline([
        ('tfidf', vectorizer),
        ('classifier', classifier)
    ])
    
    # Hu·∫•n luy·ªán m√¥ h√¨nh
    pipeline.fit(X_train, y_train)
    
    # D·ª± ƒëo√°n tr√™n t·∫≠p validation
    y_pred = pipeline.predict(X_val)
    
    # T√≠nh accuracy
    accuracy = accuracy_score(y_val, y_pred)
    
    return accuracy

def train_and_evaluate_models():
    # T·∫°o th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ n·∫øu ch∆∞a c√≥
    os.makedirs("models", exist_ok=True)
    
    # T·∫£i d·ªØ li·ªáu
    print("ƒêang t·∫£i d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω...")
    try:
        train_data = pd.read_csv("data/train.csv")
        val_data = pd.read_csv("data/val.csv")
        test_data = pd.read_csv("data/test.csv")
    except FileNotFoundError:
        print("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu. Vui l√≤ng ch·∫°y ti·ªÅn x·ª≠ l√Ω tr∆∞·ªõc!")
        return
    
    # Ghi nh·∫≠n th√¥ng tin dataset qua MLflow (cho to√†n b·ªô qu√° tr√¨nh)
    mlflow.set_experiment("Traditional_Models_All")
    with mlflow.start_run(run_name="multiple_models_training"):
        mlflow.log_param("train_samples", len(train_data))
        mlflow.log_param("validation_samples", len(val_data))
        mlflow.log_param("test_samples", len(test_data))
        
        # Danh s√°ch c√°c classifier mu·ªën th·ª≠
        classifiers = ["logistic_regression", "svm", "naive_bayes"]
        
        # D·ªØ li·ªáu train v√† validation ri√™ng
        X_train = train_data['text']
        y_train = train_data['label']
        X_val = val_data['text']
        y_val = val_data['label']
        
        # D·ªØ li·ªáu train full (train + validation)
        X_train_full = pd.concat([train_data['text'], val_data['text']])
        y_train_full = pd.concat([train_data['label'], val_data['label']])
        
        # Duy·ªát qua t·ª´ng lo·∫°i m√¥ h√¨nh
        for classifier_name in classifiers:
            print(f"\nüîç B·∫Øt ƒë·∫ßu tuning cho m√¥ h√¨nh {classifier_name}...")
            study = optuna.create_study(direction="maximize", study_name=f"traditional_models_{classifier_name}")
            study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val, classifier_name), n_trials=5)
            
            best_params = study.best_params
            best_accuracy = study.best_value
            mlflow.log_param(f"{classifier_name}_best_params", best_params)
            mlflow.log_metric(f"{classifier_name}_best_validation_accuracy", best_accuracy)
            
            print(f"Hyperparameter t·ªët nh·∫•t cho {classifier_name}: {best_params}")
            print(f"ƒê·ªô ch√≠nh x√°c t·ªët nh·∫•t tr√™n t·∫≠p validation: {best_accuracy:.4f}")
            
            # X√¢y d·ª±ng l·∫°i m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n best_params
            max_features = best_params["max_features"]
            ngram_range = best_params["ngram_range"]
            vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)
            
            if classifier_name == "logistic_regression":
                classifier = LogisticRegression(C=best_params["C"], max_iter=1000, random_state=42)
            elif classifier_name == "svm":
                classifier = LinearSVC(C=best_params["C"], random_state=42, max_iter=1000)
            elif classifier_name == "naive_bayes":
                classifier = MultinomialNB(alpha=best_params["alpha"])
            
            # T·∫°o pipeline cho m√¥ h√¨nh t·ªët nh·∫•t
            pipeline = Pipeline([
                ('tfidf', vectorizer),
                ('classifier', classifier)
            ])
            
            # Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n t·∫≠p full (train + validation)
            print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è Hu·∫•n luy·ªán m√¥ h√¨nh {classifier_name} tr√™n t·∫≠p train v√† validation...")
            pipeline.fit(X_train_full, y_train_full)
            
            # ƒê√°nh gi√° tr√™n t·∫≠p test
            print(f"ƒêang ƒë√°nh gi√° m√¥ h√¨nh {classifier_name} tr√™n t·∫≠p test...")
            y_pred = pipeline.predict(test_data['text'])
            test_accuracy = accuracy_score(test_data['label'], y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(
                test_data['label'], y_pred, average='binary'
            )
            
            mlflow.log_metric(f"{classifier_name}_test_accuracy", test_accuracy)
            mlflow.log_metric(f"{classifier_name}_test_precision", precision)
            mlflow.log_metric(f"{classifier_name}_test_recall", recall)
            mlflow.log_metric(f"{classifier_name}_test_f1", f1)
            
            # Sinh confusion matrix v√† l∆∞u l·∫°i
            cm = confusion_matrix(test_data['label'], y_pred)
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            plt.title(f'Confusion Matrix - {classifier_name}')
            cm_path = f"{classifier_name}_confusion_matrix.png"
            plt.savefig(cm_path)
            plt.close()  # ƒê√≥ng figure sau khi l∆∞u
            mlflow.log_artifact(cm_path)
            
            # L∆∞u model
            model_path = f"models/{classifier_name}.joblib"
            joblib.dump(pipeline, model_path)
            mlflow.sklearn.log_model(pipeline, classifier_name)
            
            # Sinh b√°o c√°o ph√¢n lo·∫°i v√† l∆∞u l·∫°i
            report = classification_report(test_data['label'], y_pred, target_names=['Negative', 'Positive'])
            report_path = f"models/{classifier_name}_report.txt"
            with open(report_path, "w") as f:
                f.write(report)
            mlflow.log_artifact(report_path)
            
            # In k·∫øt qu·∫£ ƒë√°nh gi√° ra m√†n h√¨nh
            print(f"M√¥ h√¨nh {classifier_name} ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán th√†nh c√¥ng!")
            print(f"ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p test: {test_accuracy:.4f}")
            print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
            
if __name__ == "__main__":
    train_and_evaluate_models()